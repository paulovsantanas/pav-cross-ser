{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import glob\n",
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from torchtune.modules import RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find pav-ser.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulovsantanas\u001b[0m (\u001b[33mpaulovsantanasteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/paulosantana/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "load_dotenv(dotenv_path='variables.env')\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/5427549/files/emoUERJ.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unzip emoUERJ.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR='emoUERJ'\n",
    "DEVICE='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoUERJFeatureDataset(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Position 1: actor's gender ('m' for man or 'w' for woman)\n",
    "    Positions 2 and 3: actor's id (from 01 to 04)\n",
    "    Position 4: emotion (h: happiness, a: anger, s: sadness, n: neutral)\n",
    "    Positions 5 and 6: recording identification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels=[\"h\", \"a\", \"s\", \"n\"],\n",
    "        actor_ids=[1, 2, 3, 4],\n",
    "        data_path=DATASET_DIR,\n",
    "        device=DEVICE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DEVICE = device\n",
    "        self.all_file_paths = glob.glob(\n",
    "            os.path.join(data_path, \"**\", \"*.pt\"), recursive=True\n",
    "        )\n",
    "        # Filtra o dataset por atores (útil para criar conjuntos de teste e validação)\n",
    "        self.actor_ids = actor_ids\n",
    "        self.file_paths = list(\n",
    "            filter(\n",
    "                lambda fp: int(os.path.basename(fp)[2]) in actor_ids,\n",
    "                self.all_file_paths,\n",
    "            )\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)\n",
    "\n",
    "    def _get_label_by_fp(self, fp):\n",
    "        name = os.path.basename(fp)\n",
    "        label = name[3]\n",
    "        return label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fp = self.file_paths[index]\n",
    "        label_id = self.get_label_id(self._get_label_by_fp(fp))\n",
    "        feature = torch.load(fp)\n",
    "        return feature, int(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoUERJDataset(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Position 1: actor's gender ('m' for man or 'w' for woman)\n",
    "    Positions 2 and 3: actor's id (from 01 to 04)\n",
    "    Position 4: emotion (h: happiness, a: anger, s: sadness, n: neutral)\n",
    "    Positions 5 and 6: recording identification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor,\n",
    "        labels=[\"h\", \"a\", \"s\", \"n\"],\n",
    "        actor_ids=[1, 2, 3, 4],\n",
    "        data_path=DATASET_DIR,\n",
    "        device=DEVICE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DEVICE = device\n",
    "        self.all_file_paths = glob.glob(\n",
    "            os.path.join(data_path, \"**\", \"*.wav\"), recursive=True\n",
    "        )\n",
    "        # Filtra o dataset por atores (útil para criar conjuntos de teste e validação)\n",
    "        self.actor_ids = actor_ids\n",
    "        self.file_paths = list(\n",
    "            filter(\n",
    "                lambda fp: int(os.path.basename(fp)[2]) in actor_ids,\n",
    "                self.all_file_paths,\n",
    "            )\n",
    "        )\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)\n",
    "\n",
    "    def _get_label_by_fp(self, fp):\n",
    "        name = os.path.basename(fp)\n",
    "        label = name[3]\n",
    "        return label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fp = self.file_paths[index]\n",
    "        label_id = self.get_label_id(self._get_label_by_fp(fp))\n",
    "        feature = self.feature_extractor(fp)\n",
    "        return torch.Tensor(feature).float(), int(label_id), os.path.splitext(os.path.basename(fp))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import librosa\n",
    "from torchaudio.transforms import (\n",
    "    Resample,\n",
    "    Spectrogram,\n",
    "    MelSpectrogram,\n",
    "    TimeStretch,\n",
    "    FrequencyMasking,\n",
    "    TimeMasking,\n",
    "    MelScale,\n",
    ")\n",
    "from torchaudio.functional import detect_pitch_frequency\n",
    "\n",
    "\n",
    "class Preprocessing(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, duration: float = 7, input_freq: int = 44100, resample_freq: int = 16000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_samples = int(duration * resample_freq)\n",
    "        self.resample = Resample(orig_freq=input_freq, new_freq=resample_freq)\n",
    "\n",
    "    def _to_mono(self, waveform):\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform\n",
    "\n",
    "    def _cut_if_necessary(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = waveform[:, : self.num_samples]\n",
    "        return waveform\n",
    "\n",
    "    def _right_pad_if_necessary(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - num_samples\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            waveform = torch.nn.functional.pad(waveform, last_dim_padding)\n",
    "        return waveform\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        preprocessed_waveform = self.resample(self._to_mono(waveform))\n",
    "        preprocessed_waveform = self._cut_if_necessary(preprocessed_waveform)\n",
    "        preprocessed_waveform = self._right_pad_if_necessary(preprocessed_waveform)\n",
    "        return preprocessed_waveform\n",
    "\n",
    "\n",
    "class MelFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, n_fft: int = 1024, n_mels: int = 80, sample_rate: int = 16000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spec = Spectrogram(n_fft=n_fft, power=2)\n",
    "\n",
    "        self.spec_aug = torch.nn.Sequential(\n",
    "            FrequencyMasking(freq_mask_param=80),\n",
    "            TimeMasking(time_mask_param=80),\n",
    "        )\n",
    "\n",
    "        self.mel_scale = MelScale(\n",
    "            n_mels=n_mels, sample_rate=sample_rate, n_stft=n_fft // 2 + 1\n",
    "        )\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        spec = self.spec(waveform)\n",
    "        spec = self.spec_aug(spec)\n",
    "        mel = self.mel_scale(spec)\n",
    "\n",
    "        return mel\n",
    "\n",
    "\n",
    "class ModelFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        features, _ = self.model.extract_features(waveform)\n",
    "\n",
    "        return features[-1]\n",
    "    \n",
    "class NoopExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileToFeature(torch.nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.preprocess = Preprocessing()\n",
    "        self.extract_feature = feature_extractor\n",
    "\n",
    "    def forward(self, fp: str) -> torch.Tensor:\n",
    "        x, sr = torchaudio.load(fp)\n",
    "        x = self.preprocess(x)\n",
    "        return self.extract_feature(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "#     fig, axs = plt.subplots(1, 1)\n",
    "#     axs.set_title(title or \"Spectrogram (db)\")\n",
    "#     axs.set_ylabel(ylabel)\n",
    "#     axs.set_xlabel(\"frame\")\n",
    "#     im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "#     fig.colorbar(im, ax=axs)\n",
    "#     plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = EmoUERJDataset(\n",
    "#     FileToFeature(MelFeatureExtractor()),\n",
    "#     actor_ids=[1, 2, 4]\n",
    "# )\n",
    "\n",
    "# plot_spectrogram(dataset[4][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento entre as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# dataset = EmoUERJDataset(\n",
    "#     FileToFeature(NoopExtractor()),\n",
    "#     actor_ids=[1, 2, 4]\n",
    "# )\n",
    "# temp = [{\"class\": data[1]} for data in dataset]\n",
    "# df = pd.DataFrame(temp)\n",
    "# df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train files: 185\n",
      "Total validation files: 192\n"
     ]
    }
   ],
   "source": [
    "melspec_train_dataset = EmoUERJDataset(\n",
    "    FileToFeature(MelFeatureExtractor()),\n",
    "    actor_ids=[1, 2]\n",
    ")\n",
    "print(f\"Total train files: {len(melspec_train_dataset)}\")\n",
    "melspec_valid_dataset = EmoUERJDataset(\n",
    "    FileToFeature(MelFeatureExtractor()),\n",
    "    actor_ids=[3, 4]\n",
    ")\n",
    "print(f\"Total validation files: {len(melspec_valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    melspec_train_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    batch_size=8\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    melspec_valid_dataset,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WavLM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavlm_train_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_wavlm_features',\n",
    "    actor_ids=[1, 2]\n",
    ")\n",
    "wavlm_valid_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_wavlm_features',\n",
    "    actor_ids=[3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hubert Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_train_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_hubert_features',\n",
    "    actor_ids=[1, 2]\n",
    ")\n",
    "hubert_valid_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_hubert_features',\n",
    "    actor_ids=[3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav2vec2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2_train_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_wav2vec2_features',\n",
    "    actor_ids=[1, 2]\n",
    ")\n",
    "wav2vec2_valid_dataset = EmoUERJFeatureDataset(\n",
    "    data_path='emoUERJ_wav2vec2_features',\n",
    "    actor_ids=[3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_config = ProjectConfiguration(\n",
    "    total_limit=5,  # Keep only the latest 10 checkpoints\n",
    "    automatic_checkpoint_naming=True  # Enable automatic naming\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.output_dir = f\"models/{self.model_name}\"\n",
    "        self.hub_model_id = f\"paulovsantanas/{self.model_name}\"\n",
    "\n",
    "    train_batch_size = 64\n",
    "    # eval_batch_size = 16\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 10\n",
    "    save_model_steps = 10\n",
    "    mixed_precision = \"fp16\"\n",
    "    # output_dir = f\"models/{self.model_name}\"  # the model name locally and on the HF Hub\n",
    "    # push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    # hub_model_id = f\"paulovsantanas/{self.model_name}\"  # the name of the repository to create on the HF Hub\n",
    "    # hub_private_repo = True\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "    # max_train_samples = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def train_accelerate(\n",
    "    model,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    criterium,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    config: TrainingConfig,\n",
    "    project_config: ProjectConfiguration,\n",
    "):\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"wandb\",\n",
    "        project_dir=os.path.join(config.output_dir),\n",
    "        project_config=project_config,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\n",
    "            \"pav-ser\",\n",
    "            init_kwargs={\"wandb\": {\"name\": f\"{config.model_name}_{uuid.uuid4()}\"}},\n",
    "        )\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        total=config.num_epochs, disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    progress_bar.set_description(f\"Epoch\")\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_acc = []\n",
    "        epoch_loss = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                model.train()\n",
    "\n",
    "                audio = batch[0].to(DEVICE)\n",
    "                labels = batch[1].to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(audio)\n",
    "\n",
    "                loss = criterium(outputs, labels)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                global_step += 1\n",
    "                logs = {}\n",
    "\n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Train metrics\n",
    "                    epoch_loss.append(loss.float().cpu().item())\n",
    "\n",
    "                    preds = torch.argmax(\n",
    "                        torch.nn.functional.softmax(outputs, dim=-1), dim=-1\n",
    "                    )\n",
    "                    epoch_acc.extend((preds == labels).float().cpu().numpy())\n",
    "\n",
    "                    epoch_avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "                    epoch_avg_acc = sum(epoch_acc) / len(epoch_acc)\n",
    "\n",
    "                    logs = {\n",
    "                        \"Train Average Loss\": epoch_avg_loss,\n",
    "                        \"Train Average Accuracy\": epoch_avg_acc,\n",
    "                    }\n",
    "\n",
    "                    # Validation metrics\n",
    "                    if step == len(train_dataloader) - 1:\n",
    "                        validation_loss = []\n",
    "                        validation_acc = []\n",
    "                        for audio, labels in validation_dataloader:\n",
    "                            audio = audio.to(DEVICE)\n",
    "                            labels = labels.to(DEVICE)\n",
    "\n",
    "                            validation_outputs = model(audio)\n",
    "                            validation_loss.append(\n",
    "                                criterium(validation_outputs, labels)\n",
    "                                .float()\n",
    "                                .cpu()\n",
    "                                .item()\n",
    "                            )\n",
    "\n",
    "                            validation_preds = torch.argmax(\n",
    "                                torch.nn.functional.softmax(validation_outputs, dim=-1),\n",
    "                                dim=-1,\n",
    "                            )\n",
    "                            validation_acc.extend(\n",
    "                                (validation_preds == labels).float().cpu().numpy()\n",
    "                            )\n",
    "\n",
    "                        validation_avg_loss = sum(validation_loss) / len(\n",
    "                            validation_loss\n",
    "                        )\n",
    "                        validation_avg_acc = sum(validation_acc) / len(validation_acc)\n",
    "\n",
    "                        logs[\"Validation Average Loss\"] = validation_avg_loss\n",
    "                        logs[\"Validation Average Accuracy\"] = validation_avg_acc\n",
    "\n",
    "                logs[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                logs[\"step\"] = global_step + 1\n",
    "                # logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step+1}\n",
    "                accelerator.log(logs, step=global_step)\n",
    "                # progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if (global_step + 1) % config.save_model_steps == 0 or global_step == (\n",
    "                    config.num_epochs * len(train_dataloader)\n",
    "                ) - 1:\n",
    "                    accelerator.save_state(config.output_dir)\n",
    "        progress_bar.update(1)\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 in_channels=1,\n",
    "                 out_channels=32,\n",
    "                 kernel_size=(3, 3)):\n",
    "        # Inicializa a classe Conv2DBlock que herda de nn.Module\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "        # Atributos da classe\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Define a operação de convolução\n",
    "        self.conv = nn.Conv2d(\n",
    "            self.in_channels,\n",
    "            self.out_channels,\n",
    "            stride=config[\"stride\"],\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "\n",
    "        # Define o tipo de normalização que será utilizado\n",
    "        if config[\"norm\"] == \"group_normalization\":\n",
    "            self.norm = nn.GroupNorm(self.in_channels, self.out_channels)\n",
    "        elif config[\"norm\"] == \"batch_normalization\":\n",
    "            self.norm = nn.BatchNorm2d(self.out_channels)\n",
    "        elif config[\"norm\"] == \"instance_normalization\":\n",
    "            self.norm = nn.InstanceNorm2d(self.out_channels)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "        # Define a função de ativação que será utilizada\n",
    "        if config[\"activation\"] == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif config[\"activation\"] == \"mish\":\n",
    "            self.activation = nn.Mish()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {config.activation}\")\n",
    "\n",
    "        # Define o tipo de pooling que será utilizado\n",
    "        if config[\"pooling\"]:\n",
    "            self.pooling = nn.MaxPool2d(tuple(config[\"pooling_ks\"]))\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        # Define a técnica de regularização que será utilizada\n",
    "        if config[\"dropout\"] > 0:\n",
    "            self.dropout = nn.Dropout2d(config[\"dropout\"])\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, T, num_feature]\n",
    "        x = self.conv(x)\n",
    "        # x: [B, n_filters, T, num_feature]\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        if self.pooling:\n",
    "            x = self.pooling(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        # x: [B, n_filters, T, num_feature]\n",
    "        return x\n",
    "    \n",
    "class SERCNN2D(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SERCNN2D, self).__init__()\n",
    "\n",
    "        # Setup convolutional blocks\n",
    "        conv_block_kernels = config[\"conv_block_kernels\"] # Lista de tamanhos de kernel para as camadas convolucionais\n",
    "        conv_block_kernels.reverse() # Inverte a lista para começar com o menor kernel\n",
    "        # Cria a primeira camada convolucional\n",
    "        init_layer = Conv2DBlock(config[\"conv_block\"],\n",
    "                                  out_channels=config[\"conv_block_dims\"][0],\n",
    "                                  kernel_size=tuple(conv_block_kernels.pop()))\n",
    "        conv_blocks = [init_layer] # Lista para armazenar as camadas convolucionais\n",
    "        inc = config[\"conv_block_dims\"][0] # Número de canais de entrada para a próxima camada\n",
    "        # Cria as outras camadas convolucionais\n",
    "        for dim in config[\"conv_block_dims\"][1:]:\n",
    "            kernel_size = tuple(conv_block_kernels.pop())\n",
    "            conv_blocks.append(Conv2DBlock(config[\"conv_block\"],\n",
    "                                            kernel_size=kernel_size,\n",
    "                                            in_channels=inc,\n",
    "                                            out_channels=dim))\n",
    "            inc = dim\n",
    "        self.conv_blocks_module = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        # Setup linear layers\n",
    "        _input = torch.zeros(torch.Size(ast.literal_eval(config[\"input_size\"]))).unsqueeze(0)\n",
    "\n",
    "        # Cria um tensor para determinar o tamanho de entrada da camada totalmente conectada 1\n",
    "        dummy_tensor = self.conv_blocks_module(_input)\n",
    "        fc1_in = 1\n",
    "        for s in dummy_tensor.size()[1:]:\n",
    "            fc1_in *= s\n",
    "\n",
    "        # Função para criar uma camada linear com as especificações dadas\n",
    "        def setup_linear_block(cfg, in_channels):\n",
    "            block = []\n",
    "\n",
    "            block = []\n",
    "            dim = cfg[\"dim\"]\n",
    "            block.append(nn.Linear(in_channels, dim))\n",
    "            if cfg[\"dropout\"] > 0:\n",
    "                block.append(nn.Dropout(cfg[\"dropout\"]))\n",
    "            if cfg[\"activation\"] == \"relu\":\n",
    "                block.append(nn.ReLU())\n",
    "            else:\n",
    "                raise ValueError(f'Unsupported activation: {cfg[\"activation\"]}')\n",
    "\n",
    "            return nn.Sequential(*block), dim\n",
    "\n",
    "        # Cria as camadas totalmente conectadas\n",
    "        self.fc1, fc2_in = setup_linear_block(config[\"fc1\"], fc1_in)\n",
    "        self.fc2, out_in = setup_linear_block(config[\"fc2\"], fc2_in)\n",
    "\n",
    "        # Camada de projeção para a saída\n",
    "        self.proj = nn.Linear(out_in, config[\"num_labels\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passa o sinal de entrada pelas camadas convolucionais\n",
    "        x = self.conv_blocks_module(x)\n",
    "\n",
    "        # Transforma o tensor resultante em um vetor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Passa o vetor pela camada totalmente conectada 1\n",
    "        x = self.fc1(x)\n",
    "        # Passa o vetor pela camada totalmente conectada 2\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Passa o vetor pela camada de projeção para obter a saída\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"input_size\": '(1, 80, 219)',\n",
    "        \"conv_block\": {\n",
    "            \"dropout\": 0.25,\n",
    "            \"norm\": \"batch_normalization\",\n",
    "            \"activation\": \"mish\",\n",
    "            \"pooling\": True,\n",
    "            \"pooling_ks\": [2, 2],\n",
    "            \"stride\": 1\n",
    "        },\n",
    "        \"fc1\": {\n",
    "            \"dim\": 512,\n",
    "            \"activation\": \"relu\",\n",
    "            \"dropout\": 0.25\n",
    "        },\n",
    "        \"fc2\": {\n",
    "            \"dim\": 256,\n",
    "            \"activation\": \"relu\",\n",
    "            \"dropout\": 0.25\n",
    "        },\n",
    "        \"conv_block_kernels\": [[3, 3], [3, 3], [3, 3]],\n",
    "        \"conv_block_dims\": [64, 32, 16],\n",
    "        \"num_labels\": 4\n",
    "}\n",
    "\n",
    "cnn2d_training_config = TrainingConfig('pav-cnn2d-ser')\n",
    "cnn2d_training_config.num_epochs = 50\n",
    "cnn2d_training_config.lr_warmup_steps = 100\n",
    "cnn2d_training_config.train_batch_size = 8\n",
    "cnn2d_training_config.save_model_steps = 50\n",
    "cnn2d_training_config.learning_rate = 1e-4\n",
    "\n",
    "cnn2d_model = SERCNN2D(config)\n",
    "cnn2d_optimizer = torch.optim.AdamW(cnn2d_model.parameters(), lr=cnn2d_training_config.learning_rate)\n",
    "cnn2d_lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=cnn2d_optimizer,\n",
    "    num_warmup_steps=cnn2d_training_config.lr_warmup_steps,\n",
    "    num_training_steps=cnn2d_training_config.num_epochs*len(train_dataloader),\n",
    ")\n",
    "cnn2d_criterium = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(cnn2d_model, (1, 80, 219))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accelerate(\n",
    "    cnn2d_model,\n",
    "    cnn2d_optimizer,\n",
    "    cnn2d_lr_scheduler,\n",
    "    cnn2d_criterium,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    cnn2d_training_config,\n",
    "    project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayerNorm(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, output_dim, hidden_size=200, hidden_dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_proj = self.__create_linear_block(input_dim, hidden_size, hidden_dropout)\n",
    "\n",
    "        if num_layers:\n",
    "            self.hidden_layers = []\n",
    "            for i in range(num_layers):\n",
    "                self.hidden_layers.append(\n",
    "                    self.__create_linear_block(hidden_size, hidden_size, hidden_dropout)\n",
    "                )\n",
    "            self.hidden_layers = nn.Sequential(*self.hidden_layers)\n",
    "        self.output_proj = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        y = self.output_proj(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def __create_linear_block(self, input_dim, output_dim, hidden_dropout):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.Dropout(hidden_dropout),\n",
    "            nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WavLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_training_config = TrainingConfig('mlp-wavlm')\n",
    "mlp_training_config.num_epochs = 50\n",
    "mlp_training_config.lr_warmup_steps = 100\n",
    "mlp_training_config.train_batch_size = 8\n",
    "mlp_training_config.save_model_steps = 50\n",
    "mlp_training_config.learning_rate = 1e-6\n",
    "\n",
    "wavlm_train_dataloader = torch.utils.data.DataLoader(\n",
    "    wavlm_train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=mlp_training_config.train_batch_size\n",
    ")\n",
    "wavlm_validation_dataloader = torch.utils.data.DataLoader(\n",
    "    wavlm_valid_dataset,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "dimensions_wavlm = wavlm_train_dataloader.dataset[0][0].size()\n",
    "mlp_model = MLPLayerNorm(\n",
    "    num_layers=2,\n",
    "    input_dim=(dimensions_wavlm[1] * dimensions_wavlm[2]),\n",
    "    output_dim=4,\n",
    "    hidden_size=200,\n",
    "    hidden_dropout=0.2,\n",
    "    )\n",
    "\n",
    "mlp_model.to(DEVICE)\n",
    "mlp_optimizer = torch.optim.AdamW(mlp_model.parameters(), lr=mlp_training_config.learning_rate)\n",
    "mlp_lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=mlp_optimizer,\n",
    "    num_warmup_steps=mlp_training_config.lr_warmup_steps,\n",
    "    num_training_steps=mlp_training_config.num_epochs*len(wavlm_train_dataloader),\n",
    ")\n",
    "mlp_criterium = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accelerate(\n",
    "    mlp_model,\n",
    "    mlp_optimizer,\n",
    "    mlp_lr_scheduler,\n",
    "    mlp_criterium,\n",
    "    wavlm_train_dataloader,\n",
    "    wavlm_validation_dataloader,\n",
    "    mlp_training_config,\n",
    "    project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_training_config = TrainingConfig('mlp-hubert')\n",
    "mlp_training_config.num_epochs = 50\n",
    "mlp_training_config.lr_warmup_steps = 100\n",
    "mlp_training_config.train_batch_size = 8\n",
    "mlp_training_config.save_model_steps = 50\n",
    "mlp_training_config.learning_rate = 1e-6\n",
    "\n",
    "hubert_train_dataloader = torch.utils.data.DataLoader(\n",
    "    hubert_train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=mlp_training_config.train_batch_size\n",
    ")\n",
    "hubert_validation_dataloader = torch.utils.data.DataLoader(\n",
    "    hubert_valid_dataset,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "dimensions_hubert = hubert_train_dataloader.dataset[0][0].size()\n",
    "mlp_model = MLPLayerNorm(\n",
    "    num_layers=2,\n",
    "    input_dim=(dimensions_hubert[1] * dimensions_hubert[2]),\n",
    "    output_dim=4,\n",
    "    hidden_size=200,\n",
    "    hidden_dropout=0.2,\n",
    "    )\n",
    "\n",
    "mlp_model.to(DEVICE)\n",
    "mlp_optimizer = torch.optim.AdamW(mlp_model.parameters(), lr=mlp_training_config.learning_rate)\n",
    "mlp_lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=mlp_optimizer,\n",
    "    num_warmup_steps=mlp_training_config.lr_warmup_steps,\n",
    "    num_training_steps=mlp_training_config.num_epochs*len(hubert_train_dataloader),\n",
    ")\n",
    "mlp_criterium = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/Universidade Federal de Goiás/Mestrado/pav/projeto_final/sentiment-analysis/wandb/run-20240711_013955-r7dyudn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/r7dyudn4' target=\"_blank\">mlp-hubert_5a6fee85-085a-4a48-8897-b2fb64a3bf2a</a></strong> to <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/r7dyudn4' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser/runs/r7dyudn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80c3becc3c24f1db54955ff68378807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b104b9b803b04e7697340f51be2854d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.014 MB uploaded\\r'), FloatProgress(value=0.16103914002205072, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Average Accuracy</td><td>▁▂▃▄▆▇▇█████████████████████████████████</td></tr><tr><td>Train Average Loss</td><td>█▇▇▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Average Accuracy</td><td>▁▂▃▃▆▅▆▇▇▇██▇█▇█████████▇█▇▇████████████</td></tr><tr><td>Validation Average Loss</td><td>█▇▇▇▅▅▅▄▄▃▃▃▂▂▃▂▂▁▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▂▃▅███████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Average Accuracy</td><td>1.0</td></tr><tr><td>Train Average Loss</td><td>0.29495</td></tr><tr><td>Validation Average Accuracy</td><td>0.69792</td></tr><tr><td>Validation Average Loss</td><td>0.82822</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>step</td><td>1201</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mlp-hubert_5a6fee85-085a-4a48-8897-b2fb64a3bf2a</strong> at: <a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/r7dyudn4' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser/runs/r7dyudn4</a><br/> View project at: <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_013955-r7dyudn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accelerate(\n",
    "    mlp_model,\n",
    "    mlp_optimizer,\n",
    "    mlp_lr_scheduler,\n",
    "    mlp_criterium,\n",
    "    hubert_train_dataloader,\n",
    "    hubert_validation_dataloader,\n",
    "    mlp_training_config,\n",
    "    project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wav2vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_training_config = TrainingConfig('mlp-wav2vec2-hs200')\n",
    "mlp_training_config.num_epochs = 50\n",
    "mlp_training_config.lr_warmup_steps = 100\n",
    "mlp_training_config.train_batch_size = 8\n",
    "mlp_training_config.save_model_steps = 50\n",
    "mlp_training_config.learning_rate = 1e-6\n",
    "\n",
    "wav2vec2_train_dataloader = torch.utils.data.DataLoader(\n",
    "    wav2vec2_train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=mlp_training_config.train_batch_size\n",
    ")\n",
    "wav2vec2_validation_dataloader = torch.utils.data.DataLoader(\n",
    "    wav2vec2_valid_dataset,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "dimensions_wav2vec2 = wav2vec2_train_dataloader.dataset[0][0].size()\n",
    "mlp_model = MLPLayerNorm(\n",
    "    num_layers=2,\n",
    "    input_dim=(dimensions_wav2vec2[1] * dimensions_wav2vec2[2]),\n",
    "    output_dim=4,\n",
    "    hidden_size=200,\n",
    "    hidden_dropout=0.2,\n",
    ")\n",
    "\n",
    "mlp_model.to(DEVICE)\n",
    "mlp_optimizer = torch.optim.AdamW(mlp_model.parameters(), lr=mlp_training_config.learning_rate)\n",
    "mlp_lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=mlp_optimizer,\n",
    "    num_warmup_steps=mlp_training_config.lr_warmup_steps,\n",
    "    num_training_steps=mlp_training_config.num_epochs*len(wav2vec2_train_dataloader),\n",
    ")\n",
    "mlp_criterium = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/Universidade Federal de Goiás/Mestrado/pav/projeto_final/sentiment-analysis/wandb/run-20240711_110452-b8lx5no3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/b8lx5no3' target=\"_blank\">mlp-wav2vec2-hs200_c27bd0da-2ac2-49f1-9e32-04fd997371b5</a></strong> to <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/b8lx5no3' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser/runs/b8lx5no3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2693939d014d00b735a0bbfa5a1c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n",
      "WARNING:accelerate.accelerator:Deleting 1 checkpoints to make room for new checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f44d53c775a4f75a0b48e3c28cb0f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Average Accuracy</td><td>▁▁▃▄▅▄▄▅▆▇▆▇▇▇██████████████████████████</td></tr><tr><td>Train Average Loss</td><td>██▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Average Accuracy</td><td>▁▂▂▂▃▄▄▃▅▅▆▅▆▆▇▆▆▇▇▇▇█▇███▇▇█▇██▇██▇████</td></tr><tr><td>Validation Average Loss</td><td>██▇▇▇▇▆▆▄▅▃▃▂▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▂▃▅███████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Average Accuracy</td><td>1.0</td></tr><tr><td>Train Average Loss</td><td>0.31673</td></tr><tr><td>Validation Average Accuracy</td><td>0.63021</td></tr><tr><td>Validation Average Loss</td><td>0.9282</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>step</td><td>1201</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mlp-wav2vec2-hs200_c27bd0da-2ac2-49f1-9e32-04fd997371b5</strong> at: <a href='https://wandb.ai/paulovsantanasteam/pav-ser/runs/b8lx5no3' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser/runs/b8lx5no3</a><br/> View project at: <a href='https://wandb.ai/paulovsantanasteam/pav-ser' target=\"_blank\">https://wandb.ai/paulovsantanasteam/pav-ser</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_110452-b8lx5no3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accelerate(\n",
    "    mlp_model,\n",
    "    mlp_optimizer,\n",
    "    mlp_lr_scheduler,\n",
    "    mlp_criterium,\n",
    "    wav2vec2_train_dataloader,\n",
    "    wav2vec2_validation_dataloader,\n",
    "    mlp_training_config,\n",
    "    project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "validation_preds = []\n",
    "validation_labels = []\n",
    "for features_1, labels in hubert_validation_dataloader:\n",
    "    features_1 = features_1.to(DEVICE)\n",
    "    labels = labels\n",
    "\n",
    "    validation_outputs = mlp_model(features_1)\n",
    "\n",
    "    validation_preds.extend(\n",
    "        torch.argmax(\n",
    "            torch.nn.functional.softmax(validation_outputs, dim=-1),\n",
    "            dim=-1,\n",
    "        ).cpu().numpy()\n",
    "    )\n",
    "    validation_labels.extend(labels)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(validation_labels, validation_preds)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"h\", \"a\", \"s\", \"n\"])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing features WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_wavlm = torchaudio.pipelines.WAVLM_LARGE\n",
    "wavlm = bundle_wavlm.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 349, 1024])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wavlm_dataset = EmoUERJDataset(\n",
    "#     FileToFeature(ModelFeatureExtractor(wavlm))\n",
    "# )\n",
    "wavlm_train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b4e550b34f4257a99016eb4ecf6455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'emoUERJ_wavlm_features'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for x, y, filename in tqdm(wavlm_dataset):\n",
    "    torch.save(x, f\"{folder}/{filename}.pt\", _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing features HuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_hubert = torchaudio.pipelines.HUBERT_XLARGE\n",
    "hubert = bundle_hubert.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_dataset = EmoUERJDataset(\n",
    "    FileToFeature(ModelFeatureExtractor(hubert))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 349, 1280])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubert_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98086612cadd4e9b96b30b129c423b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'emoUERJ_hubert_features'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for x, y, filename in tqdm(hubert_dataset):\n",
    "    torch.save(x, f\"{folder}/{filename}.pt\", _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing features wave2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_wav2vec2 = torchaudio.pipelines.WAV2VEC2_XLSR_2B\n",
    "wav2vec2 = bundle_wav2vec2.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2_dataset = EmoUERJDataset(\n",
    "    FileToFeature(ModelFeatureExtractor(wav2vec2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 349, 1920])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec2_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e966fbc480647ed876ff868a72b9145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'emoUERJ_wav2vec2_features'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "# wav2vec2.to('cpu')\n",
    "\n",
    "for x, y, filename in tqdm(wav2vec2_dataset):\n",
    "    # x.to(DEVICE)\n",
    "    torch.save(x, f\"{folder}/{filename}.pt\", _use_new_zipfile_serialization=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
